{
 "metadata": {
  "name": "Code, Refactor, Test"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "(Opportunities for) Good Practices in Scientific Programming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although at first glance it might look like it, this is **not an introduction** into the [Python](http://www.python.org) programming language (although we'll see a good chunk of its features along the way). We are just using Python, because it is a free and easy to grasp language that is well established in the scientific community. Anyone who is looking for an introduction to Python can find many good ones for various audiences on the web. Here are a few examples, but there are many more:\n",
      "\n",
      "* [The official Python tutorial](http://docs.python.org/tutorial)\n",
      "* [Dive into Python](http://www.diveintopython.net)\n",
      "* [Python for beginners](http://www.pythonforbeginners.com)\n",
      "* [Scientific Python lecture notes](http://scipy-lectures.github.io)\n",
      "\n",
      "This is **also not** a comprehensive overview of code refactoring and testing techniques -- many good books have been written about these topics, and this is one of them:\n",
      "\n",
      "* [Refactoring to patterns by Joshua Kerievsky](http://industriallogic.com/xp/refactoring)\n",
      "\n",
      "Instead of being a comprehensive something on something, this is merely **a protocol of a rather typical coding task** that has been, and will be, performed countless times in various flavors by many scientists. What may be different here, compared to most occasions, is that instead of stopping at the earliest possible stage that could be labeled \"task completed\", we'll spend just a few more minutes to help increase quality, longevity and re-usability of the resulting code.\n",
      "\n",
      "For this tutorial to be useful the reader (you!) should be already familiar with some programming language -- any, really. You should know what a variable and a function is, and that one can store numbers in lists and maybe arrays. If you, for example, have scripted anything in Matlab, you should have everything necessary to make sense of what follows below.\n",
      "\n",
      "**The actual task** in this example is to **write a parser for a text-based file format** produced by an [eye-tracker](). Our goal is to extract the time courses of gaze coordinates from the wealth of information in such a file and plot the spatial distribution of gaze positions across the entire recording session.\n",
      "\n",
      "We have three files to work with:\n",
      "\n",
      "- [gaze_snip1.txt](http://raw.github.com/neurodebian/psychotut/master/python/data/gaze_snip1.txt) (plus the [same file gzip-compressed](https://github.com/neurodebian/psychotut/raw/master/python/data/gaze_snip1.txt.gz)) and [gaze_snip2.txt](https://raw.github.com/neurodebian/psychotut/master/python/data/gaze_snip2.txt)\n",
      "\n",
      "  Two example files with a few dozen lines to give a sense of the variability of the (undocumented) syntax of the file format.\n",
      "\n",
      "- [gaze.txt.gz](https://raw.github.com/neurodebian/psychotut/master/python/data/gaze.txt.gz)\n",
      "\n",
      "  A compressed text file with data from a complete eye-tracking session -- several hundreds of thousands lines of raw data.\n",
      "  \n",
      "We'll start by working with the small example snippets to develop the parser and will eventually test it out on the big file with real data to see how it performs. Once this is done, we'll adjust the code to be more readable, robust and tested to increase its longevity."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preparations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To follow this tutorial, you need the files listed placed into a subdirectory named ``data`` (but read on for a more convenient way to get them instead of manual downloads). Start an [IPython]() session and successively copy the respective code bits into the IPython console. If you have a recent IPython version (from 0.13 onwards) you can also [download this notebook](https://github.com/neurodebian/psychotut/raw/master/python/Code%2C%20Refactor%2C%20Test.ipynb) into the directory that contains the ``data`` folder. You can then work on this tutorial with the fantastic [IPython notebook](http://ipython.org/notebook.html). Start a notebook server (by executing ``ipython notebook --pylab inline``) in this directory using a terminal. This will open a browser window where you can select the notebook. Afterwards you can start working directly in the browser -- pure magic.\n",
      "\n",
      "Instead of downloading everything manually you should \"clone\" the repository that contains this tutorial. This will give you all data and code files. Run ``git clone https://github.com/neurodebian/psychotut.git`` in a directory of your choice. Alternatively, you can download the repository as a [ZIP file](https://github.com/neurodebian/psychotut/archive/master.zip).\n",
      "Once cloned or extracted enter the created directory and go futher into its ``python`` subdirectory, where you can find all files and start the IPython notebook server.\n",
      "\n",
      "If you do not have IPython available it is possible to perform most tasks using a pure Python interpreter running in a terminal. However, this is **not recommended** as it is complicated, inconvenient, and will leave you with a false impression of scientific coding in Python. For interactive work in Python, use *Interactive Python*: IPython.\n",
      "\n",
      "A straightforward way to obtain a suitable computing environment for **any** platform is the [NeuroDebian]() virtual machine. Visit http://neuro.debian.net, select an operating system of your choice, and follow the installation instructions. The virtual machine will greet you with a welcome wizard when started for the first time. Work you way through it to update the system. When prompted, select 'Python scientific software stack' for installation. This will download and install additional 500MB of software, so please be patient.\n",
      "\n",
      "Depending on the version of the virtual machine, you may want to install the ``ipython-notebook`` package as well, in order to work directly with this notebook.\n",
      "\n",
      "And now we are good to go..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prototyping the Parser"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python is known for its utility for prototyping algorithms -- that is to quickly develop a rough sketch of an algorithm that actually runs to practically explore potential problems and runtime challenges. Quite often, however, scientific software never leaves this prototype stage, because it is \"good enough\" (TM) and the cost of a  proper/improved re-implementation does not outweight the perceived value of potential benefits.\n",
      "\n",
      "Here we'll do what is common (albeit not good) practice and simply start hacking...\n",
      "\n",
      "You'll quickly realize that we do not need to write much code, because we can use available libraries. Python has **a lot** of built-in libraries for all kinds of things, hence it earned the tag-line \"batteries included\". For example, Python might be the only language that has support for converting between color spaces as part of its standard library (see [colorsys](http://docs.python.org/library/colorsys.html)). In Python libraries are called modules, and they can be loaded via the *import* statement. You can see a list of all modules included in the standard Python distribution in the [library reference](http://docs.python.org/library/). One of the most frequently used modules is [os](http://docs.python.org/library/os.html). It offer access to operating system features in a cross-platform fashion. In the first code snippet we combine a directory name with the filename of our first example file to compose a relative filepath -- one that uses the correct path delimiter on all platforms. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "datafilename = os.path.join('data', 'gaze_snip1.txt')\n",
      "print datafilename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You'll notice the ``os.path.join`` construct. With this code we are accessing the ``join()`` function in the ``path`` submodule of the ``os`` system module. In Python each module has its own namespace, meaning it is OK to have several functions with the same name provided by different modules. Consequently, name collisions in Python are pretty much a non-issue when compared to the situation in C or MATLAB.\n",
      "\n",
      "If you ever need help regarding tohe purpose or types of arguments of a function, Python provides a ``help()`` function that is pretty convenient (we'll see below how this actually works). Even more convenient is that IPython can pull this documentation up automatically while you type a function call and hit the opening parentheses. It can also do autocompletion. Try it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(os.path.join)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are ready to dive deep into the fun of working with Python. Opening a file and printing its content is almost trivial: The built-in ``open()`` function takes a filename and returns a file handler that, in turn, supports to iteratively return a text file's content line by line when used in a for loop. You'll notice the lack of any parentheses. Where the body of the loop would be wrapped in parentheses in other languages, Python uses the code indentation level to determine this information. This reduces the variablity of look and feel of code written by different developers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for line in open(datafilename):\n",
      "    print line,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So how do we write that parser without access to the file format specifications -- we'll use a dangerous combination of intuition and common sense.\n",
      "\n",
      "From the content of the first example snippet we could get the idea that the first 76 lines are some kind of header. So let's modify the code to skip the first 76 lines by introducing a line counter and an ``if`` condition."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skiplines = 76\n",
      "for line in open(datafilename):\n",
      "    if skiplines > 0:\n",
      "        skiplines -= 1\n",
      "        continue\n",
      "    print line,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not bad. Actually, with the built-in [``enumerate()``](http://docs.python.org/2/library/functions.html#enumerate) function Python has a much more elegant way of adding a counter variable. The snippet below does the exact same thing as before -- just with less and more readable code. Moreover, we cannot forget incrementing the counter when the code gets more complicated later on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76:\n",
      "        continue\n",
      "    print line,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks already pretty good with just four lines of code, but there is a status message left in the output lines that we do not want. The most straightforward way to get rid of it is to amend the ``if`` clause and add another line exclusion condition. We can benefit from one of the methods that Python provides for strings (and yes, it also has [``endswith()``](http://docs.python.org/library/stdtypes.html#string-methods))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    print line,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Done -- almost. But instead of just printing the relevant lines we need to store the data. In many languages [lists](http://docs.python.org/tutorial/datastructures.html#more-on-lists) are extensible containers that can hold an arbitrary amount of data. The same holds for Python -- with a few extra features that we will ignore for now. Lists definition and access to elements works like it is shown below. This is pretty similar to other languages. Note however, the handy way to access list elements relative to the end of the list by using negative index values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l = [1, 2, 3, 4]\n",
      "print l[0]\n",
      "print l[3]\n",
      "print l[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is more than one way to extend and modify Python lists. The only one we'll need is the ``append()`` method that -- the name suggests it -- appends a single element to the end of a list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l.append(10)\n",
      "print l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That should give us all we need: We create an empty list at the start and append all relevant data lines to it while looping over all lines in the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = []\n",
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    data.append(line)\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, that worked somewhat. We have a list element per data sample, but everything is still in text form interspersed with TAB and NEWLINE control characters. This is nothing we can do computing with. Luckily Python is fully equiped with pretty versatile text processing tools. All strings have a ``split()`` method that, by default, splits a string into its whitespace-delimited segments -- just what we need."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "someline = data[0]\n",
      "lineitems = someline.split()\n",
      "lineitems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last field in every lines seems to be some sort of garbage, so we'll strip it off. Python lists support slicing (subselection of elements) using a [START:STOP:STEP] notation, where START and STEP are optional. With the aforementioned option of using negative indices we have to possible ways to select the first four elements from a five element list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print lineitems[:4]\n",
      "print lineitems[:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's merge this functionality into our existing code, and we'll be able to produce a list with an element for each data sample, where each element is a 4-item list with the numbers we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = []\n",
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    data.append(line.split()[:-1])\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try it out. We compute the duration of the recording from the first column. We have heard that this is the time stamp in milliseconds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dt = data[-1][0] - data[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Arrgh, our \"numbers\" are still text (or strings), and we cannot do meaningful substraction with them. Whoever said that \"in Python you do not need to care about data types\" was obviously wrong. Consequently, Python provides functions to convert text into basic data types such as integers (``int``) and floating point numbers (``float``)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "int('1234')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "float('1234')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For convenience we will convert all our numbers into floats. And because it is a lot to type, we'll only do it for the first three in each line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = []\n",
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    lineitems = line.split()[:-1]\n",
      "    data.append([float(lineitems[0]), float(lineitems[1]), float(lineitems[2])])\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dt = data[-1][0] - data[0][0]\n",
      "dt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally something to compute with: the recording duration of our data snippet was 12 ms. Now let's plot the gaze coordinates. The ``X`` coordinate is in the second column, the ``Y` coordinate in the third. We have a list of lists, so let's first select all samples and then the second item in each."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[:][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hmm, that did not work. What about"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[:,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This doesn't seem to be right either. Nested lists do not seem to be the right container for our data, in order to access its content in the way we want to.\n",
      "\n",
      "A more appropriate container is an ``array``. This is a (multidimensional/multi-axis) field of number of the same type. Most language offer such a data container, because it is useful for storing lots of data and accessing it at low cost. In Python the [``numpy``](http://www.numpy.org) package provides arrays -- and they are almost like in MATLAB. However, MATLAB users will appreciate the [NumPy for MATLAB users guide](http://wiki.scipy.org/NumPy_for_Matlab_Users) that pinpoints a few key differences.\n",
      "\n",
      "Thankfully, converting our list of lists into a two-dimensional array is pretty simple -- after first importing the ``numpy`` module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "darr = np.array(data)\n",
      "darr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "darr.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Arrays support additional modes of slicing (this a more complicated topic that you should [read more about](http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)), so what caused an error before now does the right thing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "darr[:,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Making a plot of the gaze trajectory over the 12 ms is now pretty easy. We are using [matplotlib](http://matplotlib.org/) and specifically its ``pyplot`` module's ``plot`` function to plot X against Y coordinates for each data sample. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as pl\n",
      "pl.plot(darr[:,1], darr[:,2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems like we have all pieces to do what we want, so let's put them together. Here is the part for data import."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafilename = os.path.join('data', 'gaze_snip1.txt')\n",
      "data = []\n",
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    lineitems = line.split()[:-1]\n",
      "    data.append([float(lineitems[0]), float(lineitems[1]), float(lineitems[2])])\n",
      "darr = np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see how it performs on our second data file. We only need to edit the file name."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafilename = os.path.join('data', 'gaze_snip2.txt')\n",
      "data = []\n",
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    lineitems = line.split()[:-1]\n",
      "    data.append([float(lineitems[0]), float(lineitems[1]), float(lineitems[2])])\n",
      "darr = np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hmm, not so good. It seems like our concept of the file format is not optimal. At some point the parser is trying to convert a text string into a floating point number. It fails and raises an exception. Exceptions, like in C++ or other language are a way to communicate errors.\n",
      "\n",
      "An interesting aspect of exceptions is that they can be intercepted and acted upon. So let's modify the code to catch this ``ValueError`` and take a look at the line that causes our program to fail."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafilename = os.path.join('data', 'gaze_snip2.txt')\n",
      "data = []\n",
      "for lineno, line in enumerate(open(datafilename)):\n",
      "    if lineno < 76 or line.startswith('SFIX'):\n",
      "        continue\n",
      "    lineitems = line.split()[:-1]\n",
      "    try:\n",
      "        data.append([float(lineitems[0]), float(lineitems[1]), float(lineitems[2])])\n",
      "    except ValueError, e:\n",
      "        print e\n",
      "        print line\n",
      "darr = np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Indeed, it doesn't look like a gaze coordinate sample at all. Let's take a look at this other file. We can use IPython's ability to execute arbitrary shell command (by using an explamation mark as a prefix), but you can also open the file in a text editor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n150 $datafilename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, this file looks significantly different form the first one. Especially our assumption that the first 76 lines are a (static) header seems to be wrong. We need to find the common pattern and adjust our code accordingly."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Refactor for Re-use"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a good point to look at our code and refactor it. Refactoring means to restructure the code, identify or establish reusable component with the goal to make the code more readable, and easier to maintain.\n",
      "\n",
      "The first annoying thing that we need to address is that we are forced to edit to parser code in order to read another file. If we would want to read two data files and process them together for further analysis, we would even need to copy the parser code. However, code copies are **always bad** as errors need to be fixed in multiple locations -- human errors are just a matter of time.\n",
      "\n",
      "It is much better to place the parser code into a reusable function. A function that takes one argument: the name of the file that is to be read."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data(filename):\n",
      "    data = []\n",
      "    for lineno, line in enumerate(open(filename)):\n",
      "        if lineno < 76 or line.startswith('SFIX'):\n",
      "            continue\n",
      "        lineitems = line.split()[:-1]\n",
      "        data.append([float(lineitems[0]), float(lineitems[1]), float(lineitems[2])])\n",
      "    return np.array(data)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loading the first data snippet is not affected by this change."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "load_data(os.path.join('data', 'gaze_snip1.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While we are at restructuring the code, let's look at the rest. We already know that our assumption of 76 header lines is wrong. Now imagine what a colleague would think that sees this code for the first time. She sees a magic number \"76\" and a magic string \"SFIX\" that cause a line to be ignored from further processing.\n",
      "\n",
      "We could make it a lot easier for our colleagues by actually spelling out what we intend to do. Let's take all our line exclusion conditions and move them into a function ``is_dataline()`` that takes line number and actual line content as arguments. Here is what it looks like now."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_dataline(lineno, line):\n",
      "    return lineno > 76 and not line.startswith('SFIX')\n",
      "    \n",
      "def load_data(filename):\n",
      "    data = []\n",
      "    for lineno, line in enumerate(open(filename)):\n",
      "        if not is_dataline(lineno, line):\n",
      "            continue\n",
      "        lineitems = line.split()[:-1]\n",
      "        data.append([float(lineitems[0]), float(lineitems[1]), float(lineitems[2])])\n",
      "    return np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this move we have removed two mystical values (76 and ``SFIX``) from our code in ``load_data()`` and at the same time made it obvious what kind of lines are supposed to be ignore. Our colleagues will appreciate this move if they ever need to work on our code.\n",
      "\n",
      "Something similar can be done with the next two lines. Here we decide what to store from each line. If we also move this code into its own function, we end up with code for ``load_data()`` that is very simple to read."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_line_items(line):\n",
      "    items = line.split()[:-1]\n",
      "    return [float(items[0]), float(items[1]), float(items[2])]\n",
      "\n",
      "def is_dataline(lineno, line):\n",
      "    return lineno > 76 and not line.startswith('SFIX')\n",
      "    \n",
      "def load_data(filename):\n",
      "    data = []\n",
      "    for lineno, line in enumerate(open(filename)):\n",
      "        if not is_dataline(lineno, line):\n",
      "            continue\n",
      "        data.append(get_line_items(line))\n",
      "    return np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But we need to remember that our nicely refactored code still doesn't work on the second file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "load_data(os.path.join('data', 'gaze_snip2.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly we are trying to process a line that should not be processed. All we need to do is to amend the exclusion conditions to get rid of such lines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_dataline(lineno, line):\n",
      "    return lineno > 76 and not line.startswith('SFIX') and not line.startswith('MSG')\n",
      "\n",
      "load_data(os.path.join('data', 'gaze_snip2.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, not quite -- but a different error at least.\n",
      "\n",
      "But before we look into this further let's take a step back and reconsider what we just did... We made an isolated change in the ``is_dataline()`` function, because we redefined what we consider a line for a gaze sample to be like. A colleague, our our future self looking at this change in six months would have had a much harder time figuring out what it is about, if the change would be hidden somewhere in the code of a long ``load_data()`` function, right?\n",
      "\n",
      "Now back to the bug. If we stare long and hard on the content of the second file, we'll see that there are many lines past the 76th that contain no relevant data. Maybe it is time for a more radical change.\n",
      "\n",
      "What could be a better criterion for identifying a line with data?\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "They all seem to start with a digit, right? So let's find a way to test for that. First, we grab the very first item on each line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_line = '4654784\t  638.4\t  271.8\t 4907.0\t...'\n",
      "first_item = good_line.split()[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And again Python has everything built-in -- you'll quickly get used to this convenience."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_item # type <dot> and hit tab\n",
      "first_item.isdigit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Actually, we can do this even simpler. Strings in Python are essentially sequences of characters and we can use the standard approach to access the first character."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print good_line[0]\n",
      "good_line[0].isdigit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That seems to do it, so we can simplify our ``is_dataline()`` condition. Again, for a future developer it will be obvious from this change that we redefined out concept of a dataline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_dataline(lineno, line):\n",
      "    return line[0].isdigit()\n",
      "\n",
      "load_data(os.path.join('data', 'gaze_snip2.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What? Let's open the file again and look at the data lines...\n",
      "\n",
      "We have hit the one common problem in almost all sciences -- missing data. This participant must have blinked, hence no coordinate data was available. Our parser needs to deal with this, hence we use a special value for this case: \"not a number\" or ``NaN``. We'll use this number whenever the conversion to float fails -- we already know that this would yield a ``ValueError`` exception, so we can intercept it and act accordingly.\n",
      "\n",
      "> Note: Some people will become nervous now. Aren't exceptions for handling exceptional cases? Aren't eye-blinks to be expected in eye-gaze recordings? Should we therefore explcitely test for the case of missing data? I'll leave the burden of that decision to you, but I personally do not feel fundamentalist today ;-)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def get_line_items(line):\n",
      "    items = line.split()[:-1]\n",
      "    try:\n",
      "        return [float(items[0]), float(items[1]), float(items[2])]\n",
      "    except ValueError:\n",
      "        return [float(items[0]), np.nan, np.nan]\n",
      "\n",
      "load_data(os.path.join('data', 'gaze_snip2.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It works, great!\n",
      "\n",
      "If you already know Python you feel my pain when you see this line where we convert three values into floats. Whenever you do the same thing to multiple elements of a list and you need to get a list back, you should really consider using a [list comprehension](http://docs.python.org/tutorial/datastructures.html#list-comprehensions). After the conversion it becomes more obvious what this line is supposed to do: convert the first three items in that list to float. If we ever want to switch to another datatype there would be only one place to edit, instead of three."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def get_line_items(line):\n",
      "    items = line.split()[:-1]\n",
      "    try:\n",
      "        return [float(i) for i in items[:3]]\n",
      "    except ValueError:\n",
      "        return [float(items[0]), np.nan, np.nan]\n",
      "\n",
      "load_data(os.path.join('data', 'gaze_snip2.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's assemble the complete code for our parser implementation. It looks pretty good. The ``load_data()`` function is pretty concise and readable. The two main peculiarities for handling this file format are factored into dedicated functions. As we no longer need the line number to check for data lines, this argument is now removed.\n",
      "\n",
      "We could take the refactoring even further, by passing the functions to filter data lines and extract line items as arguments to ``load_data()``. This would yield an even more generic implementation that could potentially handle much different file formats. However, we refrain from doing that, because an increased level of abstraction almost always comes with the cost of making the code less intuitive to understand. Moreover, we are not planing (yet) to support more format, so there is no benefit.\n",
      "\n",
      "Anyway, we can confirm that our code works on both data snippets that we have access to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_dataline(line):\n",
      "    return line[0].isdigit()\n",
      "\n",
      "def get_line_items(line):\n",
      "    items = line.split()[:-1]\n",
      "    try:\n",
      "        return [float(i) for i in items[:3]]\n",
      "    except ValueError:\n",
      "        return [float(items[0]), np.nan, np.nan]\n",
      "\n",
      "def load_data(filename):\n",
      "    data = []\n",
      "    for line in open(filename):\n",
      "        if not is_dataline(line):\n",
      "            continue\n",
      "        data.append(get_line_items(line))\n",
      "    return np.array(data)\n",
      "\n",
      "d1 = load_data(os.path.join('data', 'gaze_snip1.txt'))\n",
      "d2 = load_data(os.path.join('data', 'gaze_snip2.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, showtime: let's try the parser on a full logfile with hundreds of thousands of data samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = load_data(os.path.join('data', 'gaze.txt.gz'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Too bad. I read some kind of character salad. Let's take a look at the first line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "open(os.path.join('data', 'gaze.txt.gz')).readline()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ah, remember? The big data file is compressed.\n",
      "\n",
      "And again Python's amazing standard library comes to the rescue. The [``fileinput``](http://docs.python.org/library/fileinput.html) module provides functionality to read data from a number of sources: from files, from STDIN. What makes it even more awesome is the ``fileinput`` can automatically detect whether data is compressed and handle decompression automatically for us. How cool is that?\n",
      "\n",
      "Instead of adding this functionality directly to ``load_data()``, we'll refactor again: rename ``load_data()`` to ``load_data_in()`` and modify it to expect a handler to a file-like handler, instead of opening the file itself. In addition, we create a new ``load_data()`` function that does nothing but use ``fileinput`` to open a given data file and pass the resulting handler to ``load_data_in()``. If the advantages of this approach are not completely obvious right now, I can promise you'll see it later on. For know let me just mention there are more data sources that have a file-like interface than just files, and with this move we add the ability to read data from more than just files, while keeping our current programming interface intact.\n",
      "\n",
      "Let's check that we can read both, compressed and uncompressed version of the some file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import fileinput\n",
      "\n",
      "def load_data(filename):\n",
      "    fi = fileinput.FileInput(filename, openhook=fileinput.hook_compressed)\n",
      "    return load_data_in(fi)\n",
      "\n",
      "def load_data_in(file_like):\n",
      "    data = []\n",
      "    for line in file_like:\n",
      "        if not is_dataline(line):\n",
      "            continue\n",
      "        data.append(get_line_items(line))\n",
      "    return np.array(data)\n",
      "\n",
      "d1 = load_data(os.path.join('data', 'gaze_snip1.txt'))\n",
      "d2 = load_data(os.path.join('data', 'gaze_snip1.txt.gz'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we should really be ready for prime time. Here is the full code to load our big data file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import fileinput\n",
      "\n",
      "def is_dataline(line):\n",
      "    return line[0].isdigit()\n",
      "\n",
      "def get_line_items(line):\n",
      "    items = line.split()[:-1]\n",
      "    try:\n",
      "        return [float(i) for i in items[:3]]\n",
      "    except ValueError:\n",
      "        return [float(items[0]), np.nan, np.nan]\n",
      "\n",
      "def load_data_in(file_like):\n",
      "    data = []\n",
      "    for line in file_like:\n",
      "        if not is_dataline(line):\n",
      "            continue\n",
      "        data.append(get_line_items(line))\n",
      "    return np.array(data)\n",
      "\n",
      "def load_data(filename):\n",
      "    fi = fileinput.FileInput(filename, openhook=fileinput.hook_compressed)\n",
      "    return load_data_in(fi)\n",
      "\n",
      "data = load_data(os.path.join('data', 'gaze.txt.gz'))\n",
      "print \"Loaded %i samples\" % len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That took a moment! Let's create that heatmap plot we want. First a 2D histogram."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.histogram2d(data[:,1], data[:, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ha, it can't handle NaNs. So if we want a histogram, we need to deal with them in some way. The simplest is to remove all missing data samples from our array. A little function (with an expressive name) is all we need. Fortunately, ``numpy`` provides a function to test for the presence of ``NaN`` values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def remove_blinks(data):\n",
      "    filtered = []\n",
      "    for samp in data:\n",
      "        if not np.isnan(samp).sum():\n",
      "            filtered.append(samp)\n",
      "    return np.array(filtered)\n",
      "\n",
      "filtered_data = remove_blinks(data)\n",
      "print \"Removed %i samples\" % (len(data) -  len(filtered_data))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ouch, that also took a while. IPython has a built-in \"magic\" function that can be used to estimate the run time of individual code. it is pretty clever. it will run the respective code multiple times, but less often the longer it runs per iteration.\n",
      "\n",
      "By the way: IPython has quite a bit of [magic](http://ipython.org/ipython-doc/stable/interactive/tutorial.html) built-in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit remove_blinks(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow, this is code that runs for seconds in 2013. Imagine how long you'd have to wait in 1993!\n",
      "\n",
      "Let's make an attempt to make this a bit more efficient. Keep in mind that the [god father of programming](http://en.wikiquote.org/wiki/Donald_Knuth) taught us that \"premature optimization is the root of all evil\", but here we identified a bit of code that will run every time, and it just takes too long.\n",
      "\n",
      "Let's use a shortcut and assume that whenever the X coordinate of a sample is ``NaN`` we throw the sample away -- and there are no cases where the X cordinate is valid and the Y-coordinate is not. ADD A TEST\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_blinks_arr(data):\n",
      "    blink = np.isnan(data[:,1])\n",
      "    return data[np.logical_not(blink)]\n",
      "\n",
      "filtered_data = remove_blinks_arr(data)\n",
      "print \"Removed %i samples\" % (len(data) -  len(filtered_data))\n",
      "\n",
      "%timeit remove_blinks_arr(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wooohoo! Several orders of magnitude faster.\n",
      "\n",
      "> Side note: Whenever you see a ``for`` loop in Python code, and you can think of a valid algorithm to do the same without an explicit ``for`` loop there could be quite significant performance gains, because of the way the Python interpreter treats these loops.\n",
      "\n",
      "With the missing data removed we can now compute the heatmap. ``histogram2d()`` returns three values: the heatmap, as well as bin boundaries for X and Y axis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap, xbound, ybound = np.histogram2d(filtered_data[:,1], filtered_data[:, 2])\n",
      "print xbound\n",
      "print ybound"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I can tell you that the monitor had a resolution of 1680x1050 pixels, so this is not very useful. Let's contrain the values to the actual rectangle of screen coordinates to make a more useful plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_data(data):\n",
      "    d = remove_blinks_arr(data)\n",
      "    np.clip(d[:, 1], 0, 1680, d[:, 1])\n",
      "    np.clip(d[:, 2], 0, 1050, d[:, 2])\n",
      "    return d\n",
      "\n",
      "filtered_data = filter_data(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could do this in a dedicated function, but let's postpone this for now to finally see this plot. Notice ``pyplot``'s similarity to the MATLAB plotting facilities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heatmap, xbound, ybound = np.histogram2d(filtered_data[:,1], filtered_data[:, 2], bins=50)\n",
      "pl.figure()\n",
      "pl.imshow(heatmap, interpolation='nearest')\n",
      "pl.xticks(np.arange(len(xbound))[::5] - 0.5, xbound[::5], rotation=90)\n",
      "pl.yticks(np.arange(len(ybound))[::5] - 0.5 , ybound[::5])\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's turn this into a handy plotting helper function. I promised to show how IPython can pull up help for any function. Here is how it goes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_heatmap_plot(x, y, bins=50, tick_step=5, interpolation='nearest'):\n",
      "    \"\"\"Plot spatial distribution of gaze coordinates\n",
      "\n",
      "    This function cannot handle NaNs.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array\n",
      "      x-coordinates\n",
      "    y : array\n",
      "      y-coordinates (need to match length of x-coordinates)\n",
      "    bins : int\n",
      "      number of histogram bins\n",
      "    tick_steps : int\n",
      "      step size for labeling bin boundaries (e.g. 1: reports all,\n",
      "      5: labels every 5th)\n",
      "    interpolation : str\n",
      "      label of a supported interpolation method (e.g. 'bilinear', 'bicubic')\n",
      "      default: nearest neighbor interpolation\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    fig\n",
      "      a reference to the generated matplotlib figure\n",
      "    \"\"\"\n",
      "    heatmap, xbound, ybound = np.histogram2d(x, y, bins=bins)\n",
      "    fig = pl.figure()\n",
      "    pl.imshow(heatmap, interpolation=interpolation)\n",
      "    pl.xticks(np.arange(len(xbound))[::tick_step] - 0.5, xbound[::tick_step], rotation=90)\n",
      "    pl.yticks(np.arange(len(ybound))[::tick_step] - 0.5 , ybound[::tick_step])\n",
      "    return fig\n",
      "\n",
      "fig = make_heatmap_plot(filtered_data[:, 1], filtered_data[:, 2])\n",
      "pl.show()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A (multi-line) string right after the function definition, not assigned to any variable is consider as documentation by Python. There are some [conventions](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt) how such a ``docstring`` should look like to make it easy for IPython and other applications to do meaningful things with it. Similar docstrings can also be places at the top of module source files. Consequently, in Python, code and its documentation stay very close together -- which will hopefully help to keep to documentation more up-to-date -- hopefully.\n",
      "\n",
      "Let's access the documentation we just wrote."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TYPE THIS: make_heatmap_plot(\n",
      "help(make_heatmap_plot)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another nice feature of matplotlib should not go unnoticed. you can save any figure into a variety of formats for example PNG or SVG, where the latter is perfect for postprocessing with [inkscape](http://inkscape.org) for high-quality figures for your publications.\n",
      "\n",
      "Because we exposed a few selected parameters in our plotting function we can easily change the look of the plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = make_heatmap_plot(filtered_data[:, 1], filtered_data[:, 2], bins=5, tick_step=1, interpolation='bicubic')\n",
      "pl.savefig('heatmap.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are now at a stage where we can be quite satisfied with our work. Nicely compact and readable code -- seemingly functional.\n",
      "\n",
      "However, with a few more lines, we can make it even better and turn it into a command line tool that doesn't force people to even know about Python. To achieve this we need to restructure our code again. We split it into a module that contains all functionality. It contains all our functions, but nother actually calls them -- a classic library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pycat gaze/utils.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other part is the command line interface. It reads arguments from the command line and calls the appropriate functions accordingly. You'll see that it starts with a [shebang](http://en.wikipedia.org/wiki/Shebang_(Unix)) line that you may have seen in other scripts. It also contains a conditional that make the code execute only if soe magical ``__name__`` variable is set to ``\"__main__\"``. This is Python's way of indicating that a scirpt has been called directly and is not being \"imported\" into another script or library. The only remaining bit of new code is the use of the [``sys``](http://docs.python.org/library/sys.html) module to access command line arguments via the ``argv`` list -- similar to C and other languages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pycat plot_gaze_heatmap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result is a script that we can call (after having given it execute permissions, i.e. ``chmod +x ...``) with two arguments: the data file to load and the name of the file to store the plot into. Benefitting from ``matplotlib`` we can just change the file name to request the plot in a different graphics format (e.g. ``heatmap.png`` vs. ``heatmap.svg``). Try it!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!./plot_gaze_heatmap data/gaze.txt.gz heatmap.png"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By the way: IPython can also load external images and show them in a notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(filename='heatmap.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It this point it would be pretty easy to extend the functionality. We could use the [``csv``](http://docs.python.org/library/csv.html) module to export data into as a comma-separated-values file for further processing in something like Excel. Or export it as a MATLAB .mat file using SciPy's [``io``](http://docs.scipy.org/doc/scipy/reference/io.html) module. Or analyze it directly with [Pandas](http://pandas.pydata.org) using algorithms from [SciPy](http://www.scipy.org), or in [R](http://www.r-project.org) via the [``rpy2``](http://rpy.sourceforge.net/rpy2.html) module...\n",
      "\n",
      "But let's stop playing with the code here. It is always fun to make things more functional and/or faster, but all the time that goes into such work is in imminent danger of having been wasted. Do yourself a favor and read on!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Test, test, test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have just finished our implementation. We are pretty sure it does what it should -- at least we did our best. But how long will it stay this way? And how do we know when our software is broken and no longer does what it should so we can fix it?\n",
      "\n",
      "You might say: Oh come on, it is very simple code, what could possibly break?\n",
      "\n",
      "But is it really simple software? Are we actually able to tell? We have written no more than 100 lines of code, yet we can read data from compressed files, and produce plots in various formats. Clearly we are building on top of a much larger codebase, and nothing of this codebase is under our control, hence we will most likely be unaware of any changes to will happen over time.\n",
      "\n",
      "There are two approaches to address this problem:\n",
      "\n",
      "1. Keep everything as it was while developing.\n",
      "2. Make it possible and easy to verify that everything is working as intended.\n",
      "\n",
      "The first approach sounds like a simple solution to a complex problem that can be achieved with little cost. However, it turns out that, in the long run, this would be the worst thing to do. Here is why: it is **simply not possible**. You can do it for a while on one computer, but it already becomes difficult when moving to other machine (maybe you buy a new one). There is no guarantee that all underlying software works exactly the same way on the new machine. If you want to give your software to other people you practically can't force them to replicate your development system. They will have other software with requirements that are in conflict with yours. Or maybe in the future one of the software dependencies you rely on today is discontinued. There is a new tool that can replace it, but will it behave the same? The only way to figure it out would be to engange in an expensive manual investigation -- that you'll have no time for. Consequently, over time your software becomes useless or not trustworthy enough to be employed in the research process. Afterall, who would want risk unreliable or questionable results?\n",
      "\n",
      "That leaves us with method 2 -- test for correct function. It is a lot easier than you think it is. And while it may not have immediate benefits you quickly appreciate to extra effort after the first system upgrade that breaks it. Let's take a look.\n",
      "\n",
      "Python like many (all?) other languages has a built-in tool to check for the validatity of certain assumption. The [``assert ``](http://docs.python.org/reference/simple_stmts.html#the-assert-statement) can be used to test whether an expression is true. With this tool it is possible, although a bit pointless, to test whether numerical operation work as expected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(4 / 2 == 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You saw that there is no return value or message in case the expression evaluates to ``True``. That changes if it doesn't."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(1 + 1 == 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Regression tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the ``assert`` tool we can try to implement the simplest possible test. The idea is to build on top of all the manual checks for correct operation we did while developing the software. We run our algorithm on a known data file right after finishing development and record the output. The test is to verify that from now on the output stays exactly the same. This is often called [regression testing](http://en.wikipedia.org/wiki/Regression_testing).\n",
      "\n",
      "So let's run ``load_data()`` on the big file and save the outcome using NumPy's ``save()`` function that stores arrays in a binary format. Later on we could use the ``load()`` function to get or gold-standard conversion result back."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_checked_data = load_data(os.path.join('data', 'gaze.txt.gz'))\n",
      "np.save('hand_checked_gaze_data.npy', hand_checked_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So let's run the algorithm again, and see whether it yields the same result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_data = load_data(os.path.join('data', 'gaze.txt.gz'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The test expression should be straightforward."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(new_data == hand_checked_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not what we expected, let's investigate what happened."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "match = new_data == hand_checked_data\n",
      "match"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apparently the 'equals' operator for arrays performs an element-wise test for equality and returns a boolean array of the same size as the input. We need to modify the test to check for the number of positive \"equals\" results.\n",
      "\n",
      "> Note: We do not need to compute the product as arrays have a more convenient way to get there size. Try finding it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.prod(hand_checked_data.shape) - np.sum(match)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems like almost 60k elements do not match -- that is horrifying. Let's take a look."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_checked_data[np.logical_not(match)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again the missing values -- curse of science."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.nan == np.nan"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two ``NaN``s are not treated as equal -- afterall they aren't numbers. But does this mean we need to deal with special cases like this? Furtunately not! The NumPy authors are well aware of the importance of tests and consequently offer built-in support for test very common test case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.testing import assert_array_equal\n",
      "assert_array_equal([2,3,4], [2,3,5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that not only can a simple test be done with simple code, but we also get a detailed error message that saves us tedious manual work in case of an error. Now we can apply the same test to our actual data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert_array_equal(hand_checked_data, new_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No news is good news! It seems like our implementation get reliably generate the same result when ran twice in a row.\n",
      "\n",
      "It is good practice to collect tests like this one in dedicated source code files in a ``tests`` subdirectory of a project's source tree. Here is what such a file could look like: the test code placed into a function that needs no arguments and a call to this function if the file is executed directly (but not imported) as we have done before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pycat gaze/tests/test_regression.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this setup we can easily run our test form the command line, and could even automate it to run on a regular basis without the need for human attention -- unless something goes wrong."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "PYTHONPATH=. python gaze/tests/test_regression.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That didn't take long, did it? Now we have some \"executable confidence\" that our still code works. But we can do better."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unit tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While regression test are a good choice for test an entire processing stream from end to end, they are not very helpful when it comes to tracking down a problem, once it occurred. We will only no that something doesn't work as intended anymore and would then need to manually dig into the issue.\n",
      "\n",
      "This is where [unit tests](http://en.wikipedia.org/wiki/Unit_testing) come into play. Unit tests can be thought of as executable specification for a software component. We know what any function is supposed to do, what output it should yield given some input, when it should fail with what kind of error. Adding a unit tests is merely turning this knowledge into code. A unit test could look like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(is_dataline(\"13214122\t   .\t   .\t    0.0\t...\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "``is_dataline()`` should always yield ``True`` for any valid line of gaze coordinate samples. Likewise it should yield ``False`` for any line that does not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(not is_dataline(\"MSG\t4435590 DISPLAY_COORDS 0 0 1679 1049\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is obviously no possible (or really useful) to test our function with any possible text snippet. Instead we should focus on corner cases to make our design choices obvious. Again, think about unit tests as executable specification. Anyone inspecting our code would benefit from clear examples of correct usage and behavior of our functions. Here are two tests that are more closer to this goal than the two above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(is_dataline(\"1\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(not is_dataline(\" 1\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We choose to treat any line that starts with a digit as a valid data line, anything else is not.\n",
      "\n",
      "Out implementation of the conversion of text lines into actual sampes values is a bit more complicated. First the common case:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(get_line_items(\"1 2 3 4\") == [1, 2, 3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A side effect of our implementation is that we actually need four values on each data line. We should make this requirement obvious with a dedicated test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert(get_line_items(\"1 2 3\") == [1, 2, 3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see the error is not very informative. Fortunately, the [``nose``](https://nose.readthedocs.org/en/latest/) module (that is not part of the Python standard library provides several tools to make such tests simpler and more informative (Explore the content of nose's ``tools`` module!).\n",
      "\n",
      "> Note: the ``unittest`` module that is part of the Python standard library offers similar functionality."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nose.tools as nt\n",
      "nt.assert_equal(get_line_items(\"1 2 3\"), [1, 2, 3])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our test would need to document that we get just two values back with such input.\n",
      "\n",
      "> Note: Actually, this smells. Although we would not expect such an input at all (X coordinate without a Y coordinate), wouldn't it be more logical to raise an exception in this case? Try modifying this later on, after you have seen how to test for this further down."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nt.assert_list_equal(get_line_items(\"1 2 3\"), [1, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another aspect of our implementation is that it is ignorant in case of additional values on a line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nt.assert_list_equal(get_line_items(\"1 2 3 4 5\"), [1, 2, 3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And a few tests to document our handling of missing values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nt.assert_list_equal(get_line_items(\"1 . 3 4 5\"), [1, np.nan, np.nan])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nt.assert_list_equal(get_line_items(\"1 2 . 4 5\"), [1, np.nan, np.nan])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nt.assert_list_equal(get_line_items(\"1 . . . .\"), [1, np.nan, np.nan])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We do not explicitely handle the case of a missing timestamp. Instead we rely on the code to fail anyway in this case -- an assumption we should rather test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_line_items(\". 2 3 4 5\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nt.assert_raises(ValueError, get_line_items, \". 2 3 4 5\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we did with our regression test, we can store all our unit tests underneath the ``tests`` directory. In larger projects they should be grouped into separate file by the respective modules or other logical units they are testing. For a small project like ours it is sufficient to keep them all in one file. Here is how it could look. One function per tested component."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pycat gaze/tests/test_utils.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Nose](https://nose.readthedocs.org/en/latest/) also provides a commandline utility to execute such tests."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "nosetests gaze/tests/test_utils.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It will tell us how many tests ran, how many passed or failed and how long it took to run all of them. Nose can be asked to be more verbose, of course."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "nosetests -v gaze/tests/test_utils.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One particularly useful feature is that nose can tell us what parts of the code where actually executed by the tests. Code that doesn't run is not tested at all."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "nosetests --with-coverage --cover-package=gaze.utils gaze/tests/test_utils.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our test cover only 50% of the code. But we did not consider our regression test so far. Actually nose can automatically detect tests in a source tree. It will basically look for files and function (in those files) that start with ``test_`` (the complete rule set is a bit more complicated). So instead of specifying an individual file, we can just give it the name of our module ``gaze``.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "nosetests -v --with-coverage --cover-package=gaze.utils gaze"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And we are up to 72%! Only the plotting and the value range clipping code are still missing. At this point we'll leave it as an exercise to devlop appropriate tests for these components. But not without reemphasizing the importance of tests for all code. Even if it is just a \"smoke test\" that simply runs the code without actually verifying any output. Especially for scripting languages, where there isn't a compiler performing checks on the entire codebase at least once, critical bugs could remain undetected for ages in un-utilized code."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obviously there is a lot more to say or read about software tests. Take a look at an [introduction to nose](http://pythontesting.net/framework/nose/nose-introduction), or and alternative framework such as [pytest](http://pythontesting.net/framework/pytest/pytest-introduction). We haven't talked at all about the possibility to programatically test documentation with [doctest](http://docs.python.org/library/doctest.html).\n",
      "\n",
      "A test battery, like the one we just created, is an important asset for reliability assurance and feasibility of ongoing maintenance of software. It enables developers to quickly pinpoint and fix problems. I would even argue that without a test battery it is impossible to perform signification re-implementation/modernization work (the needs to happen for every piece of software eventually) without jepardizing validity and reliability.\n",
      "\n",
      "One final thought: If we remember that unit tests can be considered as an executable specification of a software we could write these specifications first. First think about what some piece of software should do, next write the tests, and finally implement the software itself until the entire test battery passes without error. This is known as \n",
      "[test-driven development](http://en.wikipedia.org/wiki/Test-driven_development).\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}